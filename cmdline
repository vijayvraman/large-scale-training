# Uncomment as needed and source this file

nohup env PYTHONUNBUFFERED=1 PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True accelerate launch --config_file accelerate_config.yaml train_mixtral_8x7b_moe_accelerate.py 2>&1 | TZ=America/Los_Angeles ts '[%Y-%m-%d %H:%M:%S %Z]' | tr '\r' '\n' > nohup.out &

#resume training (update checkpoint number as needed)
#nohup env PYTHONUNBUFFERED=1 PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True accelerate launch --config_file accelerate_config.yaml train_mixtral_8x7b_moe_accelerate.py --resume_from_checkpoint ./mixtral_moe_supervised/checkpoint-500 2>&1 | TZ=America/Los_Angeles ts '[%Y-%m-%d %H:%M:%S %Z]' | tr '\r' '\n' > nohup.out &

# Replace checkpoint number with epoch
#python upload_to_hub.py --model_path ./mixtral_moe_supervised/checkpoint-epoch-1 --repo_name vvijayk/mixtral-8x7b-moe-nq-finetuned

# Tensorboard
# tensorboard --logdir mixtral_moe_supervised --bind_all
