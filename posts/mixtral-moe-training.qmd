---
title: "Mixtral-8x7B MoE Training with Supervised Routing"
author: "Vijay Venkatraman"
date: "2026-01-06"
categories: [deep learning, training, mixture of experts, mixtral, supervised routing]
image: "thumbnail.png"
description: "Train Mixtral-8x7B Mixture of Experts models with supervised routing using HuggingFace Transformers, Accelerate, and DeepSpeed on multi-GPU systems"
---

## Overview

This project demonstrates **true MoE training** with Mixtral-8x7B using supervised routing based on dataset expert labels. Unlike standard approaches, this implementation leverages expert annotations to guide routing decisions during training.

**Key Specifications:**

- **Model**: Mixtral-8x7B-v0.1 (mistralai/Mixtral-8x7B-v0.1) - 46.7B parameters, 13B active per forward pass
- **Architecture**: Native MoE with 8 experts, Top-2 routing
- **Dataset**: Natural Questions (NQ) with expert annotations - 87,925 Q&A pairs
- **Hardware**: 2x NVIDIA H100 80GB GPUs
- **Training Framework**: DeepSpeed ZeRO-2 with HuggingFace Accelerate
- **Innovation**: Supervised routing with learnable 4→8 expert mapping
- **Training Time**: ~8-10 hours per epoch

## Features

- **True MoE Architecture**: Native Mixtral-8x7B with 8 experts and Top-2 routing
- **Supervised Routing**: Soft guidance using dataset expert labels with KL divergence loss
- **Learnable Mapping**: 4 dataset categories → 8 model experts (discovered during training)
- **Load Balancing**: Auxiliary loss for balanced expert utilization
- Multi-GPU distributed training using HuggingFace Accelerate
- DeepSpeed ZeRO-2 optimization for 46B parameter model
- Mixed precision training (BF16)
- Natural Questions dataset with expert routing annotations
- Automatic checkpointing and resume capability
- Comprehensive logging and monitoring with TensorBoard
- Optimized for H100 GPUs

## Requirements

- Python 3.8+
- CUDA-capable GPUs (optimized for 2x H100 80GB)
- DeepSpeed, Transformers, Accelerate

## Installation

```bash
# Install dependencies
pip install -r requirements.txt

# Verify installations
python -c "import torch; print(f'PyTorch: {torch.__version__}')"
python -c "import deepspeed; print(f'DeepSpeed: {deepspeed.__version__}')"
python -c "import accelerate; print(f'Accelerate: {accelerate.__version__}')"
```

## Dataset Format

We'll focus on the 4 reasoning-style experts:

- **Causal / Explanatory**: why, how, explain, cause
- **Comparative / Superlative**: largest, taller, vs, more than
- **Multi-hop**: multiple entities, conjunctions (and, or, between)
- **Direct Lookup**: everything else

The training script expects a JSONL file where each line contains:

```json
{"question": "Your question here", "answer": "Your answer here", "expert_label": "factual_lookup"}
```

Example (`nq_annotated_moe.jsonl`):

```json
{"question": "where did they film hot tub time machine", "answer": ["Fernie Alpine Resort"], "expert_label": "factual_lookup"}
{"question": "how many episodes in season 4 of the flash", "answer": ["23"], "expert_label": "numerical_reasoning"}
{"question": "who has won more grammy awards kelly or carrie", "answer": ["Carrie Underwood"], "expert_label": "multi_hop_reasoning"}
{"question": "why do we have daylight saving time in the us", "answer": ["to save energy"], "expert_label": "commonsense_reasoning"}
```

The `answer` field can be:

- A string: `"answer text"`
- A list: `["answer1", "answer2"]` (will be joined with commas)
- A dict: `{"text": "answer text"}` (common NQ format)

## Configuration

### Accelerate Config (`accelerate_config.yaml`)

Adjust `num_processes` to match your number of GPUs:

```yaml
num_processes: 4  # Change to your GPU count
```

### DeepSpeed Config (`deepspeed_moe_config.json`)

**Current Optimized Configuration (ZeRO Stage 2 for Mixtral-8x7B):**

```json
{
  "train_batch_size": 16,
  "train_micro_batch_size_per_gpu": 1,
  "gradient_accumulation_steps": 8,
  "zero_optimization": {
    "stage": 2,
    "offload_optimizer": {"device": "none"},
    "overlap_comm": true,
    "contiguous_gradients": true
  },
  "bf16": {"enabled": true},
  "moe": {
    "enabled": true,
    "num_experts": 8,
    "expert_capacity_factor": 1.25,
    "top_k": 2,
    "expert_parallel_size": 2
  }
}
```

**Key Configuration Details:**

| Parameter | Value | Notes |
|-----------|-------|-------|
| **ZeRO Stage** | 2 | Shards optimizer + gradients across GPUs (no CPU offload) |
| **Micro batch per GPU** | 1 | Mixtral is large (46B params) |
| **Gradient accumulation** | 8 | Accumulate over 8 steps before optimizer update |
| **Effective batch size** | 16 | 2 GPUs × 1 micro batch × 8 accumulation |
| **Sequence length** | 256 | Maximum token length (set via --max_seq_length) |
| **Precision** | bfloat16 | Better numerical stability than fp16 |
| **Num experts** | 8 | Native Mixtral architecture |
| **Top-K routing** | 2 | Each token uses 2 experts |
| **Expert parallelism** | 2 | 4 experts per GPU |

## Training

### Basic Training

```bash
accelerate launch --config_file accelerate_config.yaml train_mixtral_8x7b_moe_accelerate.py
```

### Training with Custom Options

```bash
accelerate launch --config_file accelerate_config.yaml train_mixtral_8x7b_moe_accelerate.py \
    --model_id mistralai/Mixtral-8x7B-v0.1 \
    --data_file nq_annotated_moe.jsonl \
    --output_dir ./mixtral_moe_supervised \
    --epochs 3 \
    --learning_rate 1e-5 \
    --max_seq_length 256 \
    --routing_loss_weight 0.1 \
    --per_device_batch_size 1 \
    --gradient_accumulation_steps 8 \
    --logging_steps 10 \
    --save_steps 500
```

### All Available Arguments

```bash
# Model settings
--model_id                     HuggingFace model ID (default: mistralai/Mixtral-8x7B-v0.1)
--routing_loss_weight          Routing supervision strength (default: 0.1, range: 0.05-0.5)
--disable_supervised_routing   Disable supervised routing (baseline mode)

# Data settings
--data_file                    Path to JSONL dataset (default: nq_annotated_moe.jsonl)
--max_samples                  Limit samples for testing (default: None)
--max_seq_length              Maximum sequence length (default: 512)
--max_target_length           Maximum target length (default: 128)

# Training settings
--epochs                       Number of epochs (default: 3)
--learning_rate               Learning rate (default: 1e-5)
--per_device_batch_size       Batch size per GPU (default: 1)
--gradient_accumulation_steps Gradient accumulation (default: 8)
--warmup_steps                Warmup steps (default: 200)
--max_grad_norm               Max gradient norm (default: 1.0)

# Logging and checkpointing
--output_dir                  Output directory (default: ./mixtral_moe_supervised)
--logging_steps               Log every N steps (default: 10)
--save_steps                  Save every N steps (default: 500)

# Reproducibility
--seed                        Random seed (default: 42)
```

### Quick Test Run

Test with limited samples:

```bash
accelerate launch --config_file accelerate_config.yaml train_mixtral_8x7b_moe_accelerate.py \
    --max_samples 100 \
    --epochs 1 \
    --save_steps 50
```

## Monitoring

### TensorBoard

```bash
tensorboard --logdir ./mixtral_moe_supervised
```

### GPU Monitoring

```bash
# In another terminal
watch -n 1 nvidia-smi
```

## Checkpoints

Checkpoints are saved to `{output_dir}/checkpoint-{N}/` and `{output_dir}/checkpoint-epoch-{N}/`:

```
mixtral_moe_supervised/
├── checkpoint-500/
│   ├── config.json
│   ├── model.safetensors
│   └── tokenizer files...
├── checkpoint-epoch-1/
│   └── ...
```

### Loading Checkpoints

```python
from transformers import MixtralForCausalLM, AutoTokenizer

model = MixtralForCausalLM.from_pretrained("./mixtral_moe_supervised/checkpoint-epoch-1")
tokenizer = AutoTokenizer.from_pretrained("./mixtral_moe_supervised/checkpoint-epoch-1")
```

## Configuration Optimization Journey

### Evolution of Configuration

#### Initial Setup (ZeRO Stage 3 + CPU Offloading)

**Problem**: Very slow training (~7.5 hours per epoch)

```json
{
  "train_micro_batch_size_per_gpu": 1,
  "gradient_accumulation_steps": 16,
  "zero_optimization": {
    "stage": 3,
    "offload_optimizer": {"device": "cpu"},
    "offload_param": {"device": "cpu"}
  }
}
```

- **GPU Usage**: Only 10 GB (12% utilization)
- **Speed**: ~1.57 it/s
- **Bottleneck**: CPU ↔ GPU transfers for parameters
- **Total iterations**: 43,963 per epoch

#### Why ZeRO Stage 2 Failed Initially

**OOM Error**: `torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.38 GiB`

```
GPU has 79.19 GiB capacity, 9.03 GiB free
Process has 70.15 GiB in use (62.03 GiB by PyTorch)
```

**Root Cause**: Used `max_seq_length=512` which caused:

- 4x more activation memory (attention is O(seq_len²))
- Memory breakdown: 62 GB (model+optimizer+gradients) + 12 GB (activations) = 74 GB
- Exceeded 79 GB available on H100

#### Final Optimized Configuration (Current)

**Solution**: ZeRO Stage 2 with `max_seq_length=256` and `micro_batch_size=4`

```json
{
  "train_micro_batch_size_per_gpu": 4,
  "gradient_accumulation_steps": 4,
  "zero_optimization": {
    "stage": 2,
    "offload_optimizer": {"device": "none"}
  }
}
```

- **GPU Usage**: ~40-50 GB (50-60% utilization - safe margin)
- **Speed**: ~2-3 it/s (1.5-2x faster than Stage 3)
- **Training Time**: ~2-2.5 hours per epoch (3x faster)
- **Total iterations**: ~10,990 optimizer steps per epoch

### Memory Usage Breakdown (Mixtral-8x7B)

| Component | Size (BF16) | ZeRO-2 (per GPU) | ZeRO-3 w/ CPU offload |
|-----------|-------------|------------------|-----------------------|
| **Model Parameters** | ~90 GB | ~45 GB (sharded) | ~0 GB (on CPU) |
| **Gradients** | ~90 GB | ~45 GB (sharded) | ~45 GB (sharded) |
| **Optimizer States** | ~180 GB | ~90 GB (sharded) | ~0 GB (on CPU) |
| **Activations (batch=1, seq=256)** | ~20 GB | ~20 GB | ~20 GB |
| **TOTAL** | - | **~50-60 GB** | **~65 GB** |

**Note**: With Top-2 routing, only 2/8 experts are active per forward pass, reducing effective compute and memory for activations.

## Understanding Training Metrics

### Progress Bar Explanation

```
Epoch 1/1: 5% | 2239/43963 [40:21<7:22:22, 1.57it/s, loss=0.0325, lr=1.97e-05]
11/25/2025 04:41:02 - INFO - Step 140/10991 | Loss: 0.7643 | LR: 1.97e-05
```

**Two different step counts:**

- **43,963**: Total dataloader iterations (batches processed)
  - Calculated as: 87,925 samples ÷ 2 (micro_batch × num_gpus) = 43,963
- **10,991**: Total optimizer steps (weight updates)
  - Calculated as: 43,963 ÷ 4 (gradient_accumulation_steps) = 10,991

**Loss values:**

- `loss=0.0325`: Instantaneous loss for current batch (can be volatile)
- `Loss: 0.7643`: Averaged loss over last 10 steps (more reliable metric)

### Batch Size Calculation

```
Effective Batch Size = num_gpus × micro_batch_size × gradient_accumulation_steps
                     = 2 × 4 × 4
                     = 32 samples per optimizer update
```

## Key Lessons Learned

### 1. Gradient Accumulation Does NOT Increase Memory

Gradient accumulation can be increased freely without memory penalty:

- Each micro-batch is processed independently
- Gradients are accumulated **in-place** (added to existing gradient tensors)
- Previous batch data is **freed from memory** before loading next batch
- Only ONE micro-batch is in GPU memory at any time

**Example**: `gradient_accumulation_steps: 4` vs `16` uses **same memory**

### 2. Sequence Length Has Quadratic Memory Impact

Memory scales with O(seq_len²) due to self-attention mechanism:

- **256 tokens**: 256² = 65,536 attention elements per head
- **512 tokens**: 512² = 262,144 attention elements per head
- **Result**: 4x more memory for activations!

### 3. ZeRO Stage Selection Guide

| Stage | GPU Memory | Speed | Use Case |
|-------|------------|-------|----------|
| **Stage 1** | High (60-70 GB) | Fastest | Maximum memory available |
| **Stage 2** | Medium (40-50 GB) | **Fast** | **Balanced (our choice)** |
| **Stage 3 (no offload)** | Low (30-40 GB) | Medium | Medium memory constraints |
| **Stage 3 + CPU offload** | Very Low (10-20 GB) | Slow | Extreme memory constraints |

**Rule of thumb**: Use the lowest ZeRO stage that fits in your GPU memory for best performance.

### 4. CPU Offloading Trade-off

CPU offloading saves GPU memory but at a significant cost:

- **Overhead**: PCIe bandwidth bottleneck (CPU ↔ GPU transfers)
- **Speed Impact**: Can make training 2-3x slower
- **When to use**: Only when absolutely necessary (GPU memory < 40 GB for 7B models)

For our H100s with 80 GB each, CPU offloading is **wasteful** - we have plenty of GPU memory!

## Troubleshooting

### Out of Memory (OOM)

If you encounter OOM errors:

1. **Reduce micro batch size**: `train_micro_batch_size_per_gpu: 4 → 2` or `1`
2. **Increase gradient accumulation**: `gradient_accumulation_steps: 4 → 8` (keeps same effective batch)
3. **Reduce sequence length**: `--max_seq_length 256 → 128`
4. **Enable CPU offloading** (last resort):
   ```json
   "offload_optimizer": {"device": "cpu", "pin_memory": true}
   ```
5. **Switch to ZeRO Stage 3** with parameter offloading

**Memory fragmentation fix** (if you see "reserved but unallocated" warnings):

```bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
```

### Slow Training

If training is too slow:

1. **Check GPU memory usage**: Should be 50-70% utilized
   ```bash
   watch -n 2 nvidia-smi
   ```
2. **Increase micro batch size** if memory allows (more GPU usage = faster)
3. **Disable CPU offloading** if enabled
4. **Use ZeRO Stage 2** instead of Stage 3 (faster communication)
5. **Reduce logging frequency**: `--logging_steps 10 → 50`

### Multi-GPU Issues

1. Verify all GPUs are visible: `nvidia-smi`
2. Check GPU count matches `num_processes` in `accelerate_config.yaml`
3. Reconfigure if needed: `accelerate config`
4. Debug with: `NCCL_DEBUG=INFO accelerate launch ...`

## Performance Tips

1. **Maximize GPU Utilization**: Increase `train_micro_batch_size_per_gpu` until you use 60-80% of GPU memory
2. **Minimize Gradient Accumulation**: Lower values = faster training (use it only when memory-constrained)
3. **Sequence Length vs Batch Size**: Shorter sequences allow larger batch sizes
4. **Mixed Precision**: BF16 is more stable than FP16 for large models
5. **Gradient Checkpointing**: Already enabled in script - trades compute for memory
6. **Effective Batch Size Formula**:
   ```
   effective_batch = micro_batch_per_gpu × num_gpus × grad_accumulation
   ```

## File Structure

```
.
├── train_mixtral_8x7b_moe_accelerate.py  # Main training script
├── supervised_routing.py          # MoE routing module with supervised routing
├── prepare_dataset.py             # Dataset annotation with expert labels
├── test_model_loading.py          # Unit test: model loading
├── test_supervised_routing.py     # Unit test: routing module
├── accelerate_config.yaml         # Accelerate distributed config (2 GPUs)
├── deepspeed_moe_config.json      # DeepSpeed ZeRO-2 config (8 experts, Top-2)
├── nq_annotated_moe.jsonl         # Annotated NQ dataset (87,925 samples)
├── mixtral_moe_supervised/        # Output directory (checkpoints)
└── README.md                      # Comprehensive guide
```

## Dataset Preparation

The Natural Questions dataset is annotated with expert labels using heuristic-based classification:

```bash
python prepare_dataset.py
```

This creates `nq_annotated_moe.jsonl` with 4 expert types:

- **factual_lookup** (who/what/when/where questions)
- **numerical_reasoning** (how many/much, distances, percentages)
- **multi_hop_reasoning** (relational/comparison questions)
- **commonsense_reasoning** (why/reason/cause questions)

The expert labels are used for MoE routing during training.

## Citation

If you use Mixtral models, please cite:

```bibtex
@article{jiang2024mixtral,
  title={Mixtral of Experts},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
  journal={arXiv preprint arXiv:2401.04088},
  year={2024}
}
```

## License

This training code is provided as-is. Please refer to the respective licenses of:

- HuggingFace Transformers (Apache 2.0)
- DeepSpeed (MIT)
- Accelerate (Apache 2.0)
- Mixtral models (Apache 2.0)

## Quick Reference

### Current Optimized Settings

```bash
# Hardware
2x NVIDIA H100 80GB GPUs

# Model
Model: Mixtral-8x7B-v0.1 (46.7B params)
Architecture: 8 experts, Top-2 routing
Active params per forward: ~13B (2/8 experts)

# Configuration
ZeRO Stage: 2 (no CPU offload)
Micro batch per GPU: 1
Gradient accumulation: 8
Effective batch size: 16
Sequence length: 256
Precision: bfloat16
Supervised routing: Enabled (weight: 0.1)

# Performance
GPU memory usage: ~50-60 GB per GPU
Training speed: ~0.5-1.0 it/s
Training time: ~8-10 hours per epoch
```

### Common Commands

```bash
# Start training
accelerate launch --config_file accelerate_config.yaml train_mixtral_8x7b_moe_accelerate.py

# Monitor GPUs
watch -n 2 nvidia-smi

# Check training logs
tail -f nohup.out | grep "Step.*Loss"

# Kill training
pkill -f train_mixtral_8x7b_moe_accelerate.py
```

### Configuration at a Glance

| What | Current Value | Where to Change |
|------|---------------|-----------------|
| Model | Mixtral-8x7B-v0.1 | Command line: `--model_id` |
| Num experts | 8 | `deepspeed_moe_config.json` → `moe.num_experts` |
| Top-K routing | 2 | `deepspeed_moe_config.json` → `moe.top_k` |
| ZeRO Stage | 2 | `deepspeed_moe_config.json` → `zero_optimization.stage` |
| Micro batch size | 1 | `deepspeed_moe_config.json` → `train_micro_batch_size_per_gpu` |
| Gradient accumulation | 8 | `deepspeed_moe_config.json` → `gradient_accumulation_steps` |
| Sequence length | 256 | Command line: `--max_seq_length 256` |
| Routing weight | 0.1 | Command line: `--routing_loss_weight 0.1` |
| Number of GPUs | 2 | `accelerate_config.yaml` → `num_processes` |
| Learning rate | 1e-5 | Command line: `--learning_rate 1e-5` |

## Support

For issues:

- HuggingFace Accelerate: https://github.com/huggingface/accelerate
- DeepSpeed: https://github.com/microsoft/DeepSpeed
- Mixtral Models: https://huggingface.co/mistralai/Mixtral-8x7B-v0.1