---
title: "Mixtral-8x7B MoE Training with Supervised Routing"
author: "Vijay Venkatraman"
date: "2026-01-25"
categories: [deep learning, training, mixture of experts, mixtral, supervised routing]
image: "thumbnail.png"
description: "Train Mixtral-8x7B Mixture of Experts models with supervised routing using HuggingFace Transformers, Accelerate, and DeepSpeed on multi-GPU systems. Optimized for fast iteration with balanced dataset."
---

## Overview

This project demonstrates **true MoE training** with Mixtral-8x7B using supervised routing based on dataset expert labels. Unlike standard approaches, this implementation leverages expert annotations to guide routing decisions during training.

**Key Specifications:**

- **Model**: Mixtral-8x7B-v0.1 (mistralai/Mixtral-8x7B-v0.1) - 46.7B parameters, 13B active per forward pass
- **Architecture**: Native MoE with 8 experts, Top-2 routing
- **Dataset**: Natural Questions (NQ) with expert annotations - **4,837 balanced samples** (balanced version) or 87,925 samples (full)
- **Hardware**: 8x NVIDIA B200 183GB GPUs
- **Training Framework**: DeepSpeed ZeRO-2 with HuggingFace Accelerate and Expert Parallelism
- **Innovation**: Supervised routing with learnable 4→8 expert mapping
- **Training Time**: ~2-3 hours for 2 epochs (balanced dataset)

**Optimizations:**

- **Balanced Dataset**: 4,837 samples with equal representation across 4 categories (vs 91% factual in full dataset)
- **Optimized Target Length**: 32 tokens (was 128) - 4x speedup, covers 99.5% of answers
- **Optimized Input Length**: 64 tokens (was 256) - better for short Q&A format
- **Hybrid Parallelism**: 4-way Data Parallelism + 2-way Expert Parallelism
- **Result**: ~18x fewer samples + 4x faster per sample = hours instead of days

## Features

- **True MoE Architecture**: Native Mixtral-8x7B with 8 experts and Top-2 routing
- **Supervised Routing**: Soft guidance using dataset expert labels with KL divergence loss
- **Learnable Mapping**: 4 dataset categories → 8 model experts (discovered during training)
- **Load Balancing**: Auxiliary loss for balanced expert utilization
- **Cosine Annealing**: Learning rate schedule with warmup for better convergence
- Multi-GPU distributed training using HuggingFace Accelerate
- DeepSpeed ZeRO-2 optimization for 46B parameter model
- Mixed precision training (BF16)
- Natural Questions dataset with expert routing annotations
- Automatic checkpointing and resume capability
- Comprehensive logging and monitoring with TensorBoard
- Optimized for H100 GPUs

## Requirements

- Python 3.8+
- CUDA-capable GPUs (optimized for 8x B200 183GB, also works on 2x H100 80GB with adjusted config)
- DeepSpeed, Transformers, Accelerate

## Installation

```bash
# Install dependencies
pip install -r requirements.txt

# Verify installations
python -c "import torch; print(f'PyTorch: {torch.__version__}')"
python -c "import deepspeed; print(f'DeepSpeed: {deepspeed.__version__}')"
python -c "import accelerate; print(f'Accelerate: {accelerate.__version__}')"
```

## Dataset Format

### Balanced Dataset (Default)

The default dataset `nq_annotated_moe_balanced.jsonl` contains **4,837 samples** with balanced class distribution:

| Category | Samples | Percentage |
|----------|---------|------------|
| `factual_lookup` | 2,000 | 41.3% |
| `numerical_reasoning` | 1,500 | 31.0% |
| `commonsense_reasoning` | 832 | 17.2% |
| `multi_hop_reasoning` | 505 | 10.4% |

**Why balanced?** The full Natural Questions dataset is 91% factual_lookup. The balanced version provides:
- Better supervised routing training (all categories well-represented)
- 18x faster training (fewer samples)
- Stronger expert specialization signal
- Meaningful evaluation across all reasoning types

**Full dataset available:** Use `--data_file nq_annotated_moe.jsonl` for all 87,925 samples.

### Expert Categories

- **factual_lookup**: Direct fact retrieval (who/what/when/where questions)
- **numerical_reasoning**: Numerical computation (how many/much, distances, percentages)
- **multi_hop_reasoning**: Multi-step reasoning (relational/comparison questions)
- **commonsense_reasoning**: Common sense inference (why/reason/cause questions)

The training script expects a JSONL file where each line contains:

```json
{"question": "Your question here", "answer": "Your answer here", "expert_label": "factual_lookup"}
```

Example:

```json
{"question": "where did they film hot tub time machine", "answer": ["Fernie Alpine Resort"], "expert_label": "factual_lookup"}
{"question": "how many episodes in season 4 of the flash", "answer": ["23"], "expert_label": "numerical_reasoning"}
{"question": "who has won more grammy awards kelly or carrie", "answer": ["Carrie Underwood"], "expert_label": "multi_hop_reasoning"}
{"question": "why do we have daylight saving time in the us", "answer": ["to save energy"], "expert_label": "commonsense_reasoning"}
```

The `answer` field can be:

- A string: `"answer text"`
- A list: `["answer1", "answer2"]` (will be joined with commas)
- A dict: `{"text": "answer text"}` (common NQ format)

## Configuration

### Accelerate Config (`accelerate_config.yaml`)

Adjust `num_processes` to match your number of GPUs:

```yaml
num_processes: 8  # Change to your GPU count (currently 8x B200)
```

### DeepSpeed Config (`deepspeed_moe_config_stage2.json`)

**Current Optimized Configuration (ZeRO Stage 2 for 8x B200 GPUs):**

```json
{
  "train_micro_batch_size_per_gpu": 1,
  "gradient_accumulation_steps": 8,
  "zero_optimization": {
    "stage": 2,
    "offload_optimizer": {
      "device": "cpu",
      "pin_memory": true
    },
    "overlap_comm": true,
    "reduce_bucket_size": 50000000.0,
    "allgather_bucket_size": 50000000.0
  },
  "bf16": {"enabled": true},
  "activation_checkpointing": {
    "partition_activations": true,
    "cpu_checkpointing": false
  },
  "moe": {
    "enabled": true,
    "num_experts": 8,
    "expert_capacity_factor": 1.0,
    "top_k": 2,
    "expert_parallel_size": 2
  }
}
```

**Key Configuration Details:**

| Parameter | Value | Notes |
|-----------|-------|-------|
| **ZeRO Stage** | 2 | Shards optimizer + gradients across Data Parallel dimension (4-way) |
| **CPU Optimizer Offload** | Enabled | Required to prevent OOM - saves ~43 GB/GPU |
| **Micro batch per GPU** | 1 | Conservative for memory stability |
| **Gradient accumulation** | 8 | Accumulate over 8 steps before optimizer update |
| **Effective batch size** | 64 | 8 GPUs × 1 micro batch × 8 accumulation |
| **Input sequence length** | 64 | Optimized for short Q&A (set via --max_seq_length) |
| **Target sequence length** | 32 | Optimized for NQ dataset (set via --max_target_length) |
| **Precision** | bfloat16 | Better numerical stability than fp16 |
| **Num experts** | 8 | Native Mixtral architecture |
| **Top-K routing** | 2 | Each token uses 2 experts |
| **Expert parallelism** | 2 | 4 experts per GPU, 4-way data parallelism |
| **Activation partitioning** | Enabled | Shards activations across GPUs, reduces memory by ~87.5% |

## Training

### Basic Training

```bash
accelerate launch --config_file accelerate_config.yaml train_mixtral_8x7b_moe_accelerate.py
```

### Training with Custom Options

```bash
# Balanced dataset (default, recommended)
accelerate launch --config_file accelerate_config.yaml train_mixtral_8x7b_moe_accelerate.py \
    --model_id mistralai/Mixtral-8x7B-v0.1 \
    --data_file nq_annotated_moe_balanced.jsonl \
    --output_dir ./mixtral_moe_supervised \
    --epochs 2 \
    --learning_rate 1e-5 \
    --max_seq_length 64 \
    --max_target_length 32 \
    --routing_loss_weight 0.1 \
    --per_device_batch_size 1 \
    --gradient_accumulation_steps 8 \
    --logging_steps 10 \
    --save_steps 500

# Full unbalanced dataset (87,925 samples, ~20 hours)
accelerate launch --config_file accelerate_config.yaml train_mixtral_8x7b_moe_accelerate.py \
    --data_file nq_annotated_moe.jsonl \
    --epochs 1
```

### All Available Arguments

```bash
# Model settings
--model_id                     HuggingFace model ID (default: mistralai/Mixtral-8x7B-v0.1)
--routing_loss_weight          Routing supervision strength (default: 0.1, range: 0.05-0.5)
--disable_supervised_routing   Disable supervised routing (baseline mode)

# Data settings
--data_file                    Path to JSONL dataset (default: nq_annotated_moe_balanced.jsonl)
--max_samples                  Limit samples for testing (default: None)
--max_seq_length              Maximum input sequence length (default: 64)
--max_target_length           Maximum target/answer length (default: 32, covers 99.5% of NQ data)

# Training settings
--epochs                       Number of epochs (default: 2)
--learning_rate               Learning rate (default: 1e-5)
--per_device_batch_size       Batch size per GPU (default: 1)
--gradient_accumulation_steps Gradient accumulation (default: 8)
--warmup_steps                Warmup steps (default: 200)
--max_grad_norm               Max gradient norm (default: 1.0)

# Logging and checkpointing
--output_dir                  Output directory (default: ./mixtral_moe_supervised)
--logging_steps               Log every N steps (default: 10)
--save_steps                  Save every N steps (default: 500)

# Reproducibility
--seed                        Random seed (default: 42)
```

### Quick Test Run

Test with limited samples:

```bash
accelerate launch --config_file accelerate_config.yaml train_mixtral_8x7b_moe_accelerate.py \
    --max_samples 100 \
    --epochs 1 \
    --save_steps 50
```

## Monitoring

### TensorBoard

```bash
tensorboard --logdir ./mixtral_moe_supervised
```

### GPU Monitoring

```bash
# In another terminal
watch -n 1 nvidia-smi
```

## Checkpoints

Checkpoints are saved to `{output_dir}/checkpoint-{N}/` and `{output_dir}/checkpoint-epoch-{N}/`:

```
mixtral_moe_supervised/
├── checkpoint-500/
│   ├── config.json
│   ├── model.safetensors
│   └── tokenizer files...
├── checkpoint-epoch-1/
│   └── ...
```

### Loading Checkpoints

```python
from transformers import MixtralForCausalLM, AutoTokenizer

model = MixtralForCausalLM.from_pretrained("./mixtral_moe_supervised/checkpoint-epoch-1")
tokenizer = AutoTokenizer.from_pretrained("./mixtral_moe_supervised/checkpoint-epoch-1")
```

## Configuration Optimization Journey

### Evolution of Configuration

#### From 2x H100 to 8x B200

**Previous Setup (2x H100 80GB):**
- Dataset: 87,925 samples (imbalanced: 91% factual)
- Sequence lengths: input=256, target=128
- Expert Parallelism: 2-way (1 expert per GPU)
- Training time: ~8-10 hours per epoch
- Memory: ~50-60 GB per GPU

**Current Optimized Setup (8x B200 183GB):**

#### Key Optimizations

**1. Dataset Balancing**
- Reduced from 87,925 to 4,837 balanced samples
- Better class distribution: 41% factual, 31% numerical, 17% commonsense, 10% multi-hop
- **Benefit**: 18x faster iteration, better routing training

**2. Optimized Sequence Lengths**
- **Target length**: 128 → 32 tokens (based on dataset analysis: 99th percentile = 24 tokens)
- **Input length**: 256 → 64 tokens (optimized for short Q&A)
- **Benefit**: ~4x speedup from reduced padding and activation memory

**3. Hybrid Parallelism**
- **Expert Parallelism (EP)**: 2-way (4 experts per GPU)
- **Data Parallelism (DP)**: 4-way (8 GPUs ÷ 2 EP = 4 DP)
- **Benefit**: Higher throughput from increased data parallelism

**4. CPU Optimizer Offload**
- Required to prevent OOM with EP=2
- Offloads ~43 GB optimizer states per GPU to CPU
- Reduces GPU usage from ~160 GB to ~111.5 GB
- **Trade-off**: ~30-40% overhead but necessary for stability

**5. Activation Partitioning**
- Shards activations across 8 GPUs
- Reduces activation memory by ~87.5%
- **Benefit**: Enables larger models without CPU activation offload (which is 2-10x slower)

#### Final Configuration Results

```json
{
  "train_micro_batch_size_per_gpu": 1,
  "gradient_accumulation_steps": 8,
  "zero_optimization": {
    "stage": 2,
    "offload_optimizer": {"device": "cpu", "pin_memory": true}
  }
}
```

- **GPU Usage**: ~111.5 GB / 183 GB (61% utilization, safe margin)
- **Speed**: ~1-2 it/s
- **Training Time**: ~2-3 hours for 2 epochs (balanced dataset)
- **Effective batch size**: 64 (1 × 8 × 8 GPUs)

### Memory Usage Breakdown (Mixtral-8x7B on 8x B200)

| Component | Total Size | Per GPU (ZeRO-2 + EP=2) |
|-----------|-----------|-------------------------|
| **Model Parameters** | ~86 GB | ~86 GB (not sharded by ZeRO-2) |
| **Gradients** | ~86 GB | ~21.5 GB (4-way DP sharded) |
| **Optimizer States** | ~172 GB | **Offloaded to CPU (~43 GB/GPU)** |
| **Activations** (batch=1, input=64, target=32) | ~32 GB | ~4 GB (partitioned 8-way) |
| **Total GPU** | - | **~111.5 GB / 183 GB** |
| **Total CPU** | - | **~43 GB (optimizer per GPU)** |

**Hybrid Parallelism Layout:**

```
GPU Setup: 8x NVIDIA B200 (183GB each)
├── Data Parallel Groups: 4
│   ├── DP Group 0: GPUs [0,1]
│   ├── DP Group 1: GPUs [2,3]
│   ├── DP Group 2: GPUs [4,5]
│   └── DP Group 3: GPUs [6,7]
├── Expert Parallel Groups: 2
│   ├── Expert Group 0: GPUs [0,2,4,6] → Experts [0,1,2,3]
│   └── Expert Group 1: GPUs [1,3,5,7] → Experts [4,5,6,7]
└── Total Batch Size: 64 (1 micro × 8 accum × 8 GPUs)
```

**Key Memory Features:**
- **CPU Optimizer Offload**: Required to prevent OOM - saves ~43 GB GPU per GPU by storing Adam states in CPU
- **Activation Partitioning**: Shards activations across 8 GPUs, reduces activation memory by ~87.5%
- **Optimized Sequence Lengths**: input_len=64, target_len=32 minimizes activation memory
- **Memory Headroom**: ~71.5 GB free per GPU for safety margin
- With expert parallelism (EP=2), each GPU holds 4 of 8 experts; Top-2 routing activates 2 experts per forward pass

## Understanding Training Metrics

### Progress Bar Explanation

```
Epoch 1/2: 15% | 120/604 [12:00<1:20:00, 1.5it/s, loss=0.0325, lr=1.97e-05]
01/25/2026 14:30:00 - INFO - Step 15/151 | Loss: 0.7643 | LR: 1.97e-05
```

**Two different step counts:**

- **604**: Total dataloader iterations (batches processed per GPU)
  - Calculated as: 4,837 samples ÷ 8 GPUs ÷ 1 (micro_batch) = 604 iterations per epoch
- **151**: Total optimizer steps (weight updates per epoch)
  - Calculated as: 4,837 samples ÷ 64 (global batch size) = 75 steps per epoch
  - Or: 604 iterations ÷ 8 (gradient_accumulation_steps) = 75 steps per epoch

**Loss values:**

- `loss=0.0325`: Instantaneous loss for current batch (can be volatile)
- `Loss: 0.7643`: Averaged loss over last 10 steps (more reliable metric)

### Batch Size Calculation

```
Effective Batch Size = num_gpus × micro_batch_size × gradient_accumulation_steps
                     = 8 × 1 × 8
                     = 64 samples per optimizer update
```

## Key Lessons Learned

### 1. Gradient Accumulation Does NOT Increase Memory

Gradient accumulation can be increased freely without memory penalty:

- Each micro-batch is processed independently
- Gradients are accumulated **in-place** (added to existing gradient tensors)
- Previous batch data is **freed from memory** before loading next batch
- Only ONE micro-batch is in GPU memory at any time

**Example**: `gradient_accumulation_steps: 4` vs `16` uses **same memory**

### 2. Sequence Length Has Quadratic Memory Impact

Memory scales with O(seq_len²) due to self-attention mechanism:

- **256 tokens**: 256² = 65,536 attention elements per head
- **512 tokens**: 512² = 262,144 attention elements per head
- **Result**: 4x more memory for activations!

### 3. ZeRO Stage Selection Guide

| Stage | GPU Memory | Speed | Use Case |
|-------|------------|-------|----------|
| **Stage 1** | High (60-70 GB) | Fastest | Maximum memory available |
| **Stage 2** | Medium (40-50 GB) | **Fast** | **Balanced (our choice)** |
| **Stage 3 (no offload)** | Low (30-40 GB) | Medium | Medium memory constraints |
| **Stage 3 + CPU offload** | Very Low (10-20 GB) | Slow | Extreme memory constraints |

**Rule of thumb**: Use the lowest ZeRO stage that fits in your GPU memory for best performance.

### 4. CPU Offloading Trade-off

CPU offloading saves GPU memory but at a significant cost:

- **Overhead**: PCIe bandwidth bottleneck (CPU ↔ GPU transfers)
- **Speed Impact**: Can make training 2-3x slower
- **When to use**: Only when absolutely necessary (GPU memory < 40 GB for 7B models)

For our H100s with 80 GB each, CPU offloading is **wasteful** - we have plenty of GPU memory!

## Troubleshooting

### Out of Memory (OOM)

**Current configuration already has CPU optimizer offload enabled** - this is required for stable training with EP=2.

If you still encounter OOM errors:

1. **Verify CPU offload is enabled**:
   ```bash
   grep -A 3 "offload_optimizer" deepspeed_moe_config_stage2.json
   # Should show: "device": "cpu"
   ```

2. **Reduce sequence lengths further**:
   ```bash
   bash run_training.sh --seq-length 32
   ```

3. **Switch to ZeRO Stage 3** (more aggressive offloading):
   ```bash
   # Edit accelerate_config.yaml, line 4:
   deepspeed_config_file: deepspeed_moe_config_stage3.json
   ```

4. **Increase expert parallelism** (reduces experts per GPU):
   ```json
   // Edit deepspeed_moe_config_stage2.json
   "moe": {
     "expert_parallel_size": 4  // Was 2, now EP=4, DP=2
   }
   ```

**Memory fragmentation fix** (if you see "reserved but unallocated" warnings):

```bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
```

### Slow Training

**Expected speed for current config:**
- 8x B200 (EP=2, DP=4, CPU offload): ~1-2 it/s
- Note: CPU optimizer offload adds ~30-40% overhead but is required to prevent OOM

If training is slower than expected:

1. **Check GPU utilization**: All 8 GPUs should be in use
   ```bash
   watch -n 2 nvidia-smi
   ```
2. **Verify NVLink/NVSwitch connectivity**:
   ```bash
   nvidia-smi topo -m
   ```
3. **Check for I/O bottlenecks** (slow disk):
   ```bash
   iostat -x 2
   ```
4. **Verify all 8 GPUs are being used**:
   ```bash
   grep "num_processes:" accelerate_config.yaml
   # Should show: num_processes: 8
   ```
5. **Reduce logging frequency**: `--logging_steps 10 → 50`

**Note**: Don't disable CPU optimizer offload - it's required for this configuration.

### Multi-GPU Issues

1. Verify all GPUs are visible: `nvidia-smi`
2. Check GPU count matches `num_processes` in `accelerate_config.yaml`
3. Reconfigure if needed: `accelerate config`
4. Debug with: `NCCL_DEBUG=INFO accelerate launch ...`

## Performance Tips

1. **Use Balanced Dataset**: The default balanced dataset (4,837 samples) is 18x faster than the full dataset while providing better routing training
2. **Optimize Sequence Lengths**: Current config (input=64, target=32) is already optimized for NQ dataset
3. **Monitor GPU Utilization**: Should be ~60% (111.5 GB / 183 GB) with CPU optimizer offload
4. **Mixed Precision**: BF16 is more stable than FP16 for large models
5. **Gradient Checkpointing + Activation Partitioning**: Already enabled - trades compute for memory efficiency
6. **Effective Batch Size Formula**:
   ```
   effective_batch = micro_batch_per_gpu × num_gpus × grad_accumulation
                   = 1 × 8 × 8 = 64
   ```
7. **For Faster Iteration**: Use `--max-samples 500` for quick testing
8. **Expert Parallelism Trade-off**: EP=2 gives good balance between memory and performance
   - EP=1: Higher memory per GPU, fewer iterations
   - EP=2: Current config, balanced
   - EP=4: Lower memory per GPU, more communication overhead

## File Structure

```
.
├── train_mixtral_8x7b_moe_accelerate.py  # Main training script
├── supervised_routing.py          # MoE routing module with supervised routing
├── prepare_dataset.py             # Dataset annotation with expert labels
├── test_model_loading.py          # Unit test: model loading
├── test_supervised_routing.py     # Unit test: routing module
├── accelerate_config.yaml         # Accelerate distributed config (8 GPUs)
├── deepspeed_moe_config_stage2.json # DeepSpeed ZeRO-2 config (8 experts, Top-2)
├── deepspeed_moe_config_stage3.json # DeepSpeed ZeRO-3 config (alternative)
├── nq_annotated_moe.jsonl         # Full annotated dataset (87,925 samples)
├── nq_annotated_moe_balanced.jsonl # Balanced dataset (4,837 samples) ⭐
├── mixtral_moe_supervised/        # Output directory (checkpoints)
├── run_training.sh                # Automated training script
└── README.md                      # Comprehensive guide
```

## Dataset Preparation

The Natural Questions dataset is annotated with expert labels using heuristic-based classification:

```bash
python prepare_dataset.py
```

This creates `nq_annotated_moe.jsonl` with 4 expert types:

- **factual_lookup** (who/what/when/where questions) - 80,204 samples (91.2%)
- **numerical_reasoning** (how many/much, distances, percentages) - 6,384 samples (7.3%)
- **multi_hop_reasoning** (relational/comparison questions) - 505 samples (0.6%)
- **commonsense_reasoning** (why/reason/cause questions) - 832 samples (0.9%)

The expert labels are used for MoE routing during training.

### Creating Balanced Dataset

The full dataset is heavily imbalanced. To create a balanced version:

```python
import json
import random

random.seed(42)

# Define target counts per category
target_counts = {
    'factual_lookup': 2000,
    'numerical_reasoning': 1500,
    'commonsense_reasoning': 'all',  # Take all 832
    'multi_hop_reasoning': 'all'     # Take all 505
}

# Read and organize by category
samples_by_category = {cat: [] for cat in target_counts}
with open('nq_annotated_moe.jsonl', 'r') as f:
    for line in f:
        data = json.loads(line)
        samples_by_category[data['expert_label']].append(data)

# Sample and shuffle
balanced = []
for cat, count in target_counts.items():
    available = samples_by_category[cat]
    selected = available if count == 'all' else random.sample(available, min(count, len(available)))
    balanced.extend(selected)

random.shuffle(balanced)

# Write balanced dataset
with open('nq_annotated_moe_balanced.jsonl', 'w') as f:
    for sample in balanced:
        f.write(json.dumps(sample) + '\n')
```

This creates `nq_annotated_moe_balanced.jsonl` with 4,837 samples and better class distribution.

## Learning Rate Schedule: Cosine Annealing

The training uses cosine annealing with warmup for the learning rate schedule:

```python
lr_scheduler = get_cosine_schedule_with_warmup(
    optimizer=optimizer,
    num_warmup_steps=warmup_steps,
    num_training_steps=total_training_steps,
)
```

### How It Works

1. **Warmup Phase**: Learning rate increases linearly from 0 to the specified `--learning_rate` value over the first warmup steps (default: 10% of total steps)
2. **Cosine Decay**: After warmup, the learning rate follows a cosine curve, gradually decreasing to near-zero
3. **Smoother Convergence**: Unlike linear decay, cosine annealing stays at higher learning rates longer, then decays more smoothly

### Benefits

- Better convergence compared to constant or linear decay
- Helps escape local minima during training
- Reduces risk of overfitting toward the end of training
- More stable training for large models like Mixtral
- The schedule is automatically saved and restored when resuming from checkpoints

### Visualization

The learning rate follows this pattern over training:

```
LR
^
|     /‾‾‾‾‾‾‾‾⌝
|    /          \_
|   /             \__
|  /                 \___
| /                      \_____
|/____________________________\_
0        warmup         total steps
```

## Citation

If you use Mixtral models, please cite:

```bibtex
@article{jiang2024mixtral,
  title={Mixtral of Experts},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
  journal={arXiv preprint arXiv:2401.04088},
  year={2024}
}
```

## License

This training code is provided as-is. Please refer to the respective licenses of:

- HuggingFace Transformers (Apache 2.0)
- DeepSpeed (MIT)
- Accelerate (Apache 2.0)
- Mixtral models (Apache 2.0)

## Quick Reference

### Current Optimized Settings

```bash
# Hardware
8x NVIDIA B200 183GB GPUs

# Model
Model: Mixtral-8x7B-v0.1 (46.7B params)
Architecture: 8 experts, Top-2 routing
Active params per forward: ~13B (2/8 experts)

# Dataset
Default: nq_annotated_moe_balanced.jsonl (4,837 samples)
Full: nq_annotated_moe.jsonl (87,925 samples)
Distribution: Balanced across 4 categories

# Configuration
ZeRO Stage: 2 (with CPU optimizer offload)
Expert Parallelism: 2-way (4 experts per GPU)
Data Parallelism: 4-way
Micro batch per GPU: 1
Gradient accumulation: 8
Effective batch size: 64
Input sequence length: 64
Target sequence length: 32
Precision: bfloat16
Supervised routing: Enabled (weight: 0.1)

# Performance
GPU memory usage: ~111.5 GB per GPU (183 GB available)
CPU memory: ~43 GB per GPU (optimizer offload)
Training speed: ~1-2 it/s
Training time: ~2-3 hours for 2 epochs (balanced dataset)
Training time: ~20 hours for 1 epoch (full dataset)
```

### Common Commands

```bash
# Start training (recommended - uses run_training.sh)
bash run_training.sh

# Or start training directly
accelerate launch --config_file accelerate_config.yaml train_mixtral_8x7b_moe_accelerate.py

# Quick test with limited samples
bash run_training.sh --max-samples 500 --epochs 1

# Use full unbalanced dataset
bash run_training.sh --data-file nq_annotated_moe.jsonl --epochs 1

# Monitor GPUs
watch -n 2 nvidia-smi

# Check training logs
tail -f mixtral_moe_supervised/training_*.log

# Monitor training (using monitor script)
bash monitor_training.sh

# Kill training
pkill -f train_mixtral_8x7b_moe_accelerate.py
```

### Configuration at a Glance

| What | Current Value | Where to Change |
|------|---------------|-----------------|
| Model | Mixtral-8x7B-v0.1 | Command line: `--model_id` |
| Dataset | nq_annotated_moe_balanced.jsonl | Command line: `--data_file` |
| Num experts | 8 | `deepspeed_moe_config_stage2.json` → `moe.num_experts` |
| Top-K routing | 2 | `deepspeed_moe_config_stage2.json` → `moe.top_k` |
| Expert parallelism | 2 | `deepspeed_moe_config_stage2.json` → `moe.expert_parallel_size` |
| ZeRO Stage | 2 | `deepspeed_moe_config_stage2.json` → `zero_optimization.stage` |
| CPU optimizer offload | Enabled | `deepspeed_moe_config_stage2.json` → `offload_optimizer.device` |
| Micro batch size | 1 | `deepspeed_moe_config_stage2.json` → `train_micro_batch_size_per_gpu` |
| Gradient accumulation | 8 | `deepspeed_moe_config_stage2.json` → `gradient_accumulation_steps` |
| Input sequence length | 64 | Command line: `--max_seq_length 64` |
| Target sequence length | 32 | Command line: `--max_target_length 32` |
| Routing weight | 0.1 | Command line: `--routing_loss_weight 0.1` |
| Number of GPUs | 8 | `accelerate_config.yaml` → `num_processes` |
| Learning rate | 1e-5 | Command line: `--learning_rate 1e-5` |
| Epochs | 2 | Command line: `--epochs 2` |

## Support

For issues:

- HuggingFace Accelerate: https://github.com/huggingface/accelerate
- DeepSpeed: https://github.com/microsoft/DeepSpeed
- Mixtral Models: https://huggingface.co/mistralai/Mixtral-8x7B-v0.1